{
  "metadata": {
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "#1. Discuss the scenarios where multithreading is preferable to multiprocessing and scenarios where multiprocessing is a better choice. \n\n\n##Ans. Multithreading and multiprocessing are both used for concurrent execution of tasks, but they have different use cases and advantages.\n\n# Multithreading is preferable in:\n\n# 1. I/O-bound tasks: When tasks involve waiting for input/output operations, such as reading/writing files, network requests, or database queries, multithreading is a better choice. Threads can wait for I/O operations without blocking other threads.\n# 2. Shared memory: When tasks need to share data and memory, multithreading is more suitable. Threads share the same memory space, making it easier to communicate and share data.\n# 3. Low overhead: Creating threads has lower overhead compared to creating processes. Threads are lighter and faster to create.\n# 4. Real-time applications: Multithreading is suitable for real-time applications where quick response times are critical.\n\n# Multiprocessing is a better choice in:\n\n# 1. CPU-bound tasks: When tasks are computationally intensive and require parallel execution, multiprocessing is a better choice. Processes can utilize multiple CPU cores, speeding up computation.\n# 2. Large datasets: When dealing with large datasets, multiprocessing can handle them more efficiently. Each process can work on a separate part of the dataset.\n# 3. Independent tasks: When tasks are independent and don't need to share data, multiprocessing is suitable. Processes can work independently without communication overhead.\n# 4. High-performance computing: Multiprocessing is suitable for high-performance computing applications, such as scientific simulations, data analysis, and machine learning.\n\n# In summary, multithreading is suitable for I/O-bound tasks, shared memory, low overhead, and real-time applications, while multiprocessing is better for CPU-bound tasks, large datasets, independent tasks, and high-performance computing.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": 1
    },
    {
      "cell_type": "code",
      "source": "#2. Describe what a process pool is and how it helps in managing multiple processes efficiently.\n\n\n##Ans. A process pool is a group of worker processes that can be used to execute tasks concurrently, improving the efficiency and performance of a program. It's a mechanism to manage multiple processes efficiently by:\n\n# 1. Reusing processes: Instead of creating a new process for each task, a process pool reuses existing processes, reducing overhead.\n# 2. Load balancing: Tasks are distributed evenly among worker processes, ensuring no single process is overwhelmed.\n# 3. Task queuing: Tasks are queued and executed as worker processes become available, preventing overloading.\n# 4. Result collection: Results from completed tasks are collected and returned to the main process.\n\n# Using a process pool helps in:\n\n# 1. Improved performance: By executing tasks concurrently, process pools speed up overall program execution.\n# 2. Efficient resource usage: Reusing processes reduces memory and CPU overhead.\n# 3. Simplified task management: Process pools handle task distribution, result collection, and error handling.\n# 4. Scalability: Process pools can be easily scaled to handle increasing workloads.\n\n# In Python, the multiprocessing module provides the Pool class to create a process pool. You can create a pool with a specified number of worker processes and use methods like map, apply, or apply_async to execute tasks concurrently.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#3. Explain what multiprocessing is and why it is used in Python programs. \n\n\n##Ans. Multiprocessing is a technique where a program uses multiple processes to execute tasks concurrently, improving overall performance and efficiency. In Python, multiprocessing is used to:\n\n# 1. Speed up computationally intensive tasks: By executing tasks in parallel across multiple CPU cores, multiprocessing reduces execution time.\n# 2. Improve responsiveness: Multiprocessing allows Python programs to remain responsive while performing time-consuming tasks in the background.\n# 3. Utilize multiple CPU cores: Modern computers often have multiple CPU cores. Multiprocessing enables Python programs to take advantage of these cores, increasing overall processing power.\n# 4. Overcome Global Interpreter Lock (GIL) limitations: Python's GIL prevents true parallel execution of threads. Multiprocessing bypasses this limitation, allowing true parallel execution.\n\n# Python's multiprocessing module provides a way to create and manage multiple processes, allowing developers to:\n\n# 1. Create multiple processes: Spawn new processes to execute tasks concurrently.\n# 2. Communicate between processes: Share data and results between processes using queues, pipes, or shared memory.\n# 3. Synchronize processes: Coordinate process execution using locks, semaphores, or barriers.\n\n# Common use cases for multiprocessing in Python include:\n\n# 1. Data processing and analysis\n# 2. Scientific computing and simulations\n# 3. Machine learning and AI\n# 4. Web scraping and crawling\n# 5. Batch processing and automation\n\n# By leveraging multiprocessing, Python developers can write more efficient, scalable, and responsive programs that take full advantage of modern computer hardware.",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#4. Write a Python program using multithreading where one thread adds numbers to a list, and another thread removes numbers from the list. Implement a mechanism to avoid race conditions using threading.Lock.  \n\n\n##Ans. Here is a Python program using multithreading where one thread adds numbers to a list, and another thread removes numbers from the list, with a mechanism to avoid race conditions using threading.Lock:\n\n\nimport threading\nimport time\nimport random\n\n# Shared list\nnumbers = []\n\n# Lock to avoid race conditions\nlock = threading.Lock()\n\n# Thread to add numbers to the list\ndef add_numbers():\n    for i in range(10):\n        with lock:  # Acquire the lock before appending\n            numbers.append(i)\n        time.sleep(random.random())  # Simulate work\n\n# Thread to remove numbers from the list\ndef remove_numbers():\n    for i in range(10):\n        with lock:  # Acquire the lock before popping\n            if numbers:\n                numbers.pop(0)\n        time.sleep(random.random())  # Simulate work\n\n# Create and start the threads\nthread1 = threading.Thread(target=add_numbers)\nthread2 = threading.Thread(target=remove_numbers)\nthread1.start()\nthread2.start()\n\n# Wait for both threads to finish\nthread1.join()\nthread2.join()\n\nprint(numbers)\n\n\n#In this program, we use a threading.Lock object to synchronize access to the shared numbers list. The with lock: statement acquires the lock before appending or popping from the list, ensuring that only one thread can modify the list at a time. This prevents race conditions and ensures thread safety. ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#5. Describe the methods and tools available in Python for safely sharing data between threads and processes. \n\n\n##Ans. Python provides several methods and tools for safely sharing data between threads and processes:\n\n# For Threads:\n\n# 1. Locks (threading.Lock): Prevent multiple threads from accessing shared data simultaneously.\n# 2. RLocks (threading.RLock): Allow a thread to acquire a lock multiple times without deadlocking.\n# 3. Semaphores (threading.Semaphore): Control access to shared resources by limiting the number of threads.\n# 4. Conditions (threading.Condition): Allow threads to wait for specific conditions before accessing shared data.\n# 5. Queues (queue.Queue): Thread-safe queues for exchanging data between threads.\n\n# For Processes:\n\n# 1. Pipes (multiprocessing.Pipe): Communication channels between processes.\n# 2. Queues (multiprocessing.Queue): Process-safe queues for exchanging data between processes.\n# 3. Shared Memory (multiprocessing.Value, multiprocessing.Array): Share memory between processes.\n# 4. Managers (multiprocessing.Manager): Create shared data structures, such as lists and dictionaries.\n# 5. Server Process (multiprocessing.Server): Create a server process to manage shared data.\n\n# Additional Tools:\n\n# 1. Manager (multiprocessing.Manager): Creates a shared namespace for data sharing.\n# 2. Proxy Objects: Allow remote access to shared data structures.\n\n# Best Practices:\n\n# 1. Minimize shared data: Reduce the amount of shared data to avoid synchronization overhead.\n# 2. Use locks and semaphores: Protect shared data with locks and semaphores.\n# 3. Avoid busy-waiting: Use conditions and queues instead of busy-waiting.\n# 4. Use high-level synchronization primitives: Prefer higher-level primitives like queues and managers over low-level locks.\n\n# By using these methods and tools, you can safely share data between threads and processes in Python, ensuring thread safety and avoiding data corruption. ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#6.  Discuss why it’s crucial to handle exceptions in concurrent programs and the techniques available for doing so.  \n\n\n##Ans. Handling exceptions in concurrent programs is crucial because:\n\n# 1. Prevents program termination: Unhandled exceptions can cause the entire program to terminate, including all threads or processes.\n# 2. Ensures thread/process safety: Unhandled exceptions can leave threads or processes in an inconsistent state, leading to unexpected behavior.\n# 3. Maintains data integrity: Unhandled exceptions can result in data corruption or loss.\n# 4. Provides error handling: Exception handling allows for meaningful error messages and recovery mechanisms.\n\n# Techniques for handling exceptions in concurrent programs:\n\n# 1. Try-except blocks: Wrap code in try-except blocks to catch and handle exceptions.\n# 2. Thread-specific exception handling: Use thread-specific exception handling mechanisms, such as threading.excepthook.\n# 3. Process-specific exception handling: Use process-specific exception handling mechanisms, such as multiprocessing.Process.error.\n# 4. Queue-based exception handling: Use queues to pass exceptions between threads or processes.\n# 5. Global exception handlers: Establish global exception handlers to catch unhandled exceptions.\n# 6. Logging and monitoring: Log and monitor exceptions to detect and diagnose issues.\n# 7. Error propagation: Propagate errors up the call stack to ensure proper handling.\n# 8. Timeouts and retries: Implement timeouts and retries to handle transient errors.\n\n# Best practices:\n\n# 1. Handle exceptions as close to the source as possible.\n# 2. Use specific exception types instead of catching general exceptions.\n# 3. Provide meaningful error messages.\n# 4. Document exception handling.\n# 5. Test exception handling.\n\n# By employing these techniques and best practices, you can ensure robust and reliable concurrent programs that handle exceptions effectively. ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#7. Create a program that uses a thread pool to calculate the factorial of numbers from 1 to 10 concurrently. Use concurrent.futures.ThreadPoolExecutor to manage the threads.  \n\n\n##Ans. Here is a Python program that uses concurrent.futures.ThreadPoolExecutor to calculate the factorial of numbers from 1 to 10 concurrently:\n\n\nimport concurrent.futures\nimport math\n\ndef factorial(n):\n    return math.factorial(n)\n\ndef main():\n    numbers = range(1, 11)\n    with concurrent.futures.ThreadPoolExecutor() as executor:\n        futures = {executor.submit(factorial, n): n for n in numbers}\n        for future in concurrent.futures.as_completed(futures):\n            n = futures[future]\n            try:\n                result = future.result()\n            except Exception as e:\n                print(f\"Error calculating factorial of {n}: {e}\")\n            else:\n                print(f\"Factorial of {n}: {result}\")\n\nif __name__ == \"__main__\":\n    main()\n\n\n# This program defines a factorial function to calculate the factorial of a number using the math.factorial function. The main function creates a list of numbers from 1 to 10 and uses ThreadPoolExecutor to execute the factorial function concurrently for each number. The results are printed as they become available.\n\n# Note that the as_completed function is used to iterate over the futures as they complete, allowing for concurrent execution and immediate printing of results. Also, error handling is included to catch any exceptions that may occur during the calculation. ",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": "#8.  Create a Python program that uses multiprocessing.Pool to compute the square of numbers from 1 to 10 in parallel. Measure the time taken to perform this computation using a pool of different sizes (e.g., 2, 4, 8 processes).  \n\n\n##Ans. Here is a Python program that uses multiprocessing.Pool to compute the square of numbers from 1 to 10 in parallel and measures the time taken to perform this computation using a pool of different sizes:\n\n\nimport multiprocessing\nimport time\n\ndef square(x):\n    return x ** 2\n\nif __name__ == \"__main__\":\n    numbers = range(1, 11)\n    \n    for pool_size in [2, 4, 8]:\n        start_time = time.time()\n        with multiprocessing.Pool(processes=pool_size) as pool:\n            results = pool.map(square, numbers)\n        end_time = time.time()\n        \n        print(f\"Pool size: {pool_size}\")\n        print(f\"Results: {results}\")\n        print(f\"Time taken: {end_time - start_time} seconds\")\n        print()\n\n\n# This program defines a square function to compute the square of a number. It then creates a list of numbers from 1 to 10 and iterates over different pool sizes (2, 4, 8). For each pool size, it measures the time taken to compute the squares of the numbers in parallel using multiprocessing.Pool and prints the results and time taken.\n\n# Note that the if __name__ == \"__main__\": guard is used to ensure that the multiprocessing code is only executed in the main process, not in the child processes.\n",
      "metadata": {
        "trusted": true
      },
      "outputs": [],
      "execution_count": null
    }
  ]
}